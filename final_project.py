# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fwzimg3WpSlBUpjjw5o_9CRYwXJI7fBQ
"""

#This project classifies a person as having a cardiovascular disease or not
#Import libraries  
import pandas as pd
import numpy as np
import seaborn
import matplotlib.pyplot as plt

#Upload the dataset
from google.colab import files
upload = files.upload()

#Store the data into a variable
df=pd.read_csv('cardio.csv',sep=';')

#Print the first 7 rows of data
df.head(7)

#Get the shape of the data
df.shape

df.cardio.value_counts()

#Count the empty values in each column
df.isnull().sum()

#Check null values
df.isnull().values.any()

#View some basic Statistics
df.describe()

#Get a count of the number of patients with a cardiovascular disease and without
df['cardio'].value_counts()

#Remove or drop the id column
df=df.drop(['id'],axis=1)

#Print the first 7 rows of data
df.head(7)

#Visualize the count
seaborn.countplot(x='gender',hue='cardio',data=df,palette='colorblind',edgecolor=seaborn.color_palette('dark',n_colors=1))

#Create a yearâ€™s column
df['year']=(df['age']/365).round(0)

#Visualize the year
df['year']

#Visualize the data
seaborn.countplot(x='year',hue='cardio',data=df,palette='colorblind',edgecolor=seaborn.color_palette('dark',n_colors=1))

#Remove or drop the year column
df=df.drop(['year'],axis=1)

df

#Remove or drop the cardio column
df=df.drop(['cardio'],axis=1)

df

#Split the data into feature data and target data
X = df.iloc[:, :-1] .values
Y = df.iloc[:, :-1] .values

#Split the data again, into 75% training data set and 25% testing data set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test, = train_test_split(X, Y, test_size=0.25, random_state = 1)

#Feature Scaling
#Scale the values in the data to be values between 0 and 1 inclusive
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Import the required libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Generate some random data for classification
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create the Random Forest Classifier model with 100 trees
rf_model = RandomForestClassifier(n_estimators=100)

#Train the model on the training data
rf_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = rf_model.predict(X_test)

#Evaluate the model's accuracy
accuracy = rf_model.score(X_test, y_test)
print("Accuracy:", accuracy)

#Import the required libraries
from sklearn.tree import DecisionTreeClassifier

#Train the model on the training data
dtc=DecisionTreeClassifier()
dtc.fit(xtrain,ytrain)

# Make predictions on the testing data
dtc.predict(xtest)

#Evaluate the model's accuracy
dtc.score(xtest,ytest)

#Import the required libraries
from sklearn.linear_model import LogisticRegression

lor=LogisticRegression()

#Train the model on the training data
lor.fit(xtrain,ytrain)

# Make predictions on the testing data
lor.predict(xtest)

#Evaluate the model's accuracy
lor.score(xtest,ytest)